<!DOCTYPE html>
<html lang="en-gb"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Activation Memory Analysis | NAN blog</title>
<meta property="og:title" content="Activation Memory Analysis | NAN blog" />
<meta name="twitter:title" content="Activation Memory Analysis | NAN blog" />
<meta itemprop="name" content="Activation Memory Analysis | NAN blog" />
<meta name="application-name" content="Activation Memory Analysis | NAN blog" />
<meta property="og:site_name" content="NAN blog" />

<meta name="description" content="Record of Learning Journey">
<meta itemprop="description" content="Record of Learning Journey" />
<meta property="og:description" content="Record of Learning Journey" />
<meta name="twitter:description" content="Record of Learning Journey" />

<meta property="og:locale" content="en-gb" />
<meta name="language" content="en-gb" />

  <link rel="alternate" hreflang="en-gb" href="https://theflash010.github.io/posts/activation-memory-analysis/" title="English" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2025-03-18T11:53:10&#43;0800 />
    <meta property="article:published_time" content=2025-03-18T11:53:10&#43;0800 />
    <meta property="og:url" content="https://theflash010.github.io/posts/activation-memory-analysis/" />

    
    <meta property="og:article:author" content="Nan Z" />
    <meta property="article:author" content="Nan Z" />
    <meta name="author" content="Nan Z" />
    
    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Activation Memory Analysis",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2025-03-18",
        "description": "",
        "wordCount":  178 ,
        "mainEntityOfPage": "True",
        "dateModified": "2025-03-18",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "NAN blog"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.145.0">

    
    <meta property="og:url" content="https://theflash010.github.io/posts/activation-memory-analysis/">
  <meta property="og:site_name" content="NAN blog">
  <meta property="og:title" content="Activation Memory Analysis">
  <meta property="og:description" content="分析使用不同模型并行策略，模型训练时**激活值**的显存占用情况 分析前提
Note that “activations” in this paper refers to any tensor that is created in the forward pass and is necessary for gradient computation during back-propagation. As a result, this excludes the main parameters of the model and optimizer state, but, for example, includes the mask used by the dropout operation. （dropout mask是被包括在里面的） We also assume that the network and the activations are stored in a 16-bit floating point format and therefore each element requires 2 bytes for storage. The only exceptions are the dropout masks which only require a single byte per element. (除了dropout mask是1byte外，其余都是2 bytes) Baseline 不使用任何模型并行策略 上图是一个Transformer架构图，基于这个图分析显存占用情况（单layer）:">
  <meta property="og:locale" content="en_gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-18T11:53:10+08:00">
    <meta property="article:modified_time" content="2025-03-18T11:53:10+08:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Activation Memory Analysis">
  <meta name="twitter:description" content="分析使用不同模型并行策略，模型训练时**激活值**的显存占用情况 分析前提
Note that “activations” in this paper refers to any tensor that is created in the forward pass and is necessary for gradient computation during back-propagation. As a result, this excludes the main parameters of the model and optimizer state, but, for example, includes the mask used by the dropout operation. （dropout mask是被包括在里面的） We also assume that the network and the activations are stored in a 16-bit floating point format and therefore each element requires 2 bytes for storage. The only exceptions are the dropout masks which only require a single byte per element. (除了dropout mask是1byte外，其余都是2 bytes) Baseline 不使用任何模型并行策略 上图是一个Transformer架构图，基于这个图分析显存占用情况（单layer）:">


    

    <link rel="canonical" href="https://theflash010.github.io/posts/activation-memory-analysis/">
    <link href="/style.min.2d921c18cf1ec555ffc03d59a8adc211c402c68c930c27d6a0c306ab175a8d09.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="https://theflash010.github.io/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
    
</head>
<body data-theme = "auto" class="notransition">

<script src="/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="https://theflash010.github.io/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/posts/">
                        Posts
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/about/">
                        About
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Activation Memory Analysis</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-03-18T11:53:10&#43;08:00" itemprop="datePublished"> 18 Mar 2025 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b>Table of Contents</b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#baseline-不使用任何模型并行策略">Baseline 不使用任何模型并行策略</a></li>
    <li><a href="#tp张量并行">TP（张量并行）</a>
      <ul>
        <li><a href="#为什么dropout的输入可以并行">为什么Dropout的输入可以并行？</a></li>
      </ul>
    </li>
    <li><a href="#tpsp张量并行和序列并行共用">TP+SP（张量并行和序列并行共用）</a>
      <ul>
        <li><a href="#有关tpsp的通信量问题">有关TP+SP的通信量问题</a></li>
      </ul>
    </li>
    <li><a href="#pptpsp流水并行序列并行张量并行共用">PP+TP+SP（流水并行，序列并行，张量并行共用）</a></li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <h1 id="分析使用不同模型并行策略模型训练时激活值的显存占用情况">分析使用不同模型并行策略，模型训练时**<font color=red>激活值</font>**的显存占用情况</h1>
<p>分析前提</p>
<ol>
<li>Note that “activations” in this paper refers to any tensor that is created in the forward pass and is necessary for gradient computation during back-propagation. As a result, this excludes the main parameters of the model and optimizer state, but, for example, includes the mask used by the dropout operation.           （dropout mask是被包括在里面的）</li>
<li>We also assume that the network and the activations are stored in a 16-bit floating point format and therefore each element requires 2 bytes for storage. The only exceptions are the dropout masks which only require a single byte per element.    (除了dropout mask是1byte外，其余都是2 bytes)</li>
</ol>
<h2 id="baseline-不使用任何模型并行策略">Baseline 不使用任何模型并行策略</h2>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317032832928.png" alt="image-20250317032832928" style="zoom:50%;" />
<p>上图是一个Transformer架构图，基于这个图分析显存占用情况（单layer）:</p>
<div style="display: flex;">
  <div><img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317033316766.png" alt="image-20250317033316766" style="zoom:70%;" /></div>
  <div><img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317033344993.png" alt="image-20250317033344993" style="zoom:70%;" /></div>
</div>
<h2 id="tp张量并行">TP（张量并行）</h2>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317034346038.png" alt="image-20250317034346038" style="zoom: 80%;" />
<p>上图是Shoeybi et al., 2019使用TP的架构图，以此来进行分析（单layer，图中f是通信算子）</p>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317034525663.png" alt="image-20250317034525663" style="zoom:50%;" />
<p>公式中</p>
<p>10 = LayerNorm1输入（2sbh）+ LayerNorm1输出/SelfAttention输入(2sbh) + Dropout1输出(sbh) + LayerNorm2输入（2sbh）+ LayerNorm2输出/Linear2输入(2sbh) + Dropout2输出(sbh)</p>
<p>除了上述的不可并行张量，其余均可利用张量并行均分到不同设备</p>
<h3 id="为什么dropout的输入可以并行">为什么Dropout的输入可以并行？</h3>
<p>因为Dropout的输入实际是Linear的输出，Linear的结果实际上还是由TP均分得出来的，所以是可并行的激活值</p>
<h2 id="tpsp张量并行和序列并行共用">TP+SP（张量并行和序列并行共用）</h2>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317055624127.png" alt="image-20250317055624127" style="zoom:80%;" />
<p>上图是使用TP+SP的架构图，以此来进行分析（单layer，g是通信算子）</p>
<p>LayerNorm和Dropout是序列独立的，所以在TP的基础上，使用SP来并行LayerNorm和Dropout，进一步减少激活值的显存占用</p>
<p>g通信算子=TP中f通信算子+SP中通信算子</p>
<p>以MLP为例，计算流程如下:</p>
<table>
  <tr>
    <td><img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317060120843.png" alt="image-20250317060120843" style="zoom:50%;" /></td> 
    <td><img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317060943870.png" alt="image-20250317060943870" style="zoom:50%;" /></td>
    </tr>
</table>
<p>公式(3)中上标表示按照什么维度划分</p>
<p>s:序列  h:隐空间  c:列  r:行</p>
<p>g是all-gather（正传）reduce-scatter（反传）   g&rsquo;是reduce-scatter（正传） all-gather（反传）</p>
<p>从公式可以看出使用TP+SP，除了Y是要所有计算节点都完整拥有的，其他激活值都可以分散在计算节点上。</p>
<p>进一步改进，Y依然分散在不同机器上，在反传时，额外进行一次all-gather操作。all-gather可以和Yi的梯度计算重叠，也就是每个机器上先算现有的Y的一部分，消除延迟影响，等其他机器的Y传过来继续算剩余梯度</p>
<p>综上，TP+SP的显存占用如下</p>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317061558666.png" alt="image-20250317061558666" style="zoom:67%;" />
<h3 id="有关tpsp的通信量问题">有关TP+SP的通信量问题</h3>
<p>一次正传反传</p>
<p>仅使用TP的通信量是2f+2f&rsquo;=4all-reduce</p>
<p>使用TP+SP=4g+4g&rsquo;=4all-gather+4reduce-scatter</p>
<p>在ring all_reduce的实现中使用的是all-gather+reduce-scatter，所以实际上TP+SP的通信量和纯TP是相同的</p>
<h2 id="pptpsp流水并行序列并行张量并行共用">PP+TP+SP（流水并行，序列并行，张量并行共用）</h2>
<p>不同的流水并行策略，带来的激活值显存占用不同，这里考虑1F1B策略时的显存使用情况</p>
<p>流水并行存在显存占用不均的问题，stage越靠前需要保留的激活值越多，第一阶段需要保留所有stage的激活值，用于反传计算梯度</p>
<p>这里使用的b是microbatch size，前面的b是正常的batch size</p>
<p>假设一共L个layer，有p个stage，仅考虑第一个stage机器的显存使用情况，由于要保存所有stage的激活值，所以公式如下:</p>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250318100939782.png" alt="image-20250318100939782" style="zoom:50%;" />
<p>其实就是所有层数（L）*单层显存使用（TP+SP），只是这里的b是microbatch_size</p>
<p>如图2所示，除了迭代的Transformer块，还包括 开头的Position and word embeddings，dropout；结尾的layer-norm，output layer projection into vocabulary</p>
<p>Position and word embeddings不需要占用空间</p>
<p>dropout需要mask空间sbhp/t   （因为有p份mask需要被保存）</p>
<p>layernorm的输入输出一共2*2sbh/t=4bsh/t</p>
<p>将layernorm的结果投影为vocabulary，cross entropy loss需要元素为4bytes，所以这部分要 4sbv/t</p>
<p>由于只考虑第一阶段的显存使用，如果p!=1，那么结尾处的layer-norm，output layer projection into vocabulary就不归第一阶段管</p>
<p>综上，由于开头和结尾对第一阶段带来的额外的显存使用如下</p>
<img src="C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250318103609264.png" alt="image-20250318103609264" style="zoom:50%;" />
<p>由于这部分extra的显存占用非常少，可以忽略不计，公式(5)就够用了</p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/theflash010" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="https://space.bilibili.com/106734387?spm_id_from=333.1007.0.0" target="_blank" rel="noopener noreferrer me"
    title="Bilibili">
    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" xmlns="http://www.w3.org/2000/svg">
    <rect x="1.3333" y="6" width="21.333" height="15.333" rx="4" ry="4"/>
    <path d="m8 12.4v1.2"/>
    <path d="m16 12.4v1.2"/>
    <path d="m5.8853 2.6667 2.6667 2.6667"/>
    <path d="m18.115 2.6667-2.6667 2.6667"/>
</svg>
</a>
<a href="/index.xml" target="_blank" rel="noopener noreferrer me"
    title="Rss">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 11a9 9 0 0 1 9 9" />
    <path d="M4 4a16 16 0 0 1 16 16" />
    <circle cx="5" cy="19" r="1" />
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2025 Nan Z.
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer><a href="#" title="Go to top" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    




    
    
        
    

    
    
        
    



    
    <script src="https://theflash010.github.io/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js" integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30&#43;lSNuSkl4QXuNyy8="></script>

    

</body>
</html>
