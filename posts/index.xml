<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on NAN blog</title>
    <link>https://theflash010.github.io/posts/</link>
    <description>Recent content in Posts on NAN blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Thu, 27 Mar 2025 14:32:49 +0800</lastBuildDate><atom:link href="https://theflash010.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Scheduling</title>
      <link>https://theflash010.github.io/posts/llm-scheduling/</link>
      <pubDate>Thu, 27 Mar 2025 14:32:49 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/llm-scheduling/</guid>
      
      <description>&lt;h1 id=&#34;有关大模型任务调度的讨论&#34;&gt;有关大模型任务调度的讨论&lt;/h1&gt;
&lt;p&gt;大模型具有一些内在的特性，导致在大模型任务调度方面会有一些新的挑战。&lt;/p&gt;
&lt;h2 id=&#34;冷启动问题&#34;&gt;冷启动问题&lt;/h2&gt;
&lt;p&gt;大模型的特点之一是参数规模非常大，常见的175B,670B等等。&lt;/p&gt;
&lt;p&gt;当系统的请求量变化时，需要针对集群开发新的调度策略，其中就包括改变集群中不同模型副本的数量，也就是进行&lt;strong&gt;扩缩容&lt;/strong&gt;操作。然而由于大模型的参数规模大，导致在gpu上加载新模型的开销非常大，甚至对于大多数大模型来说，单张gpu无法达到推理的显存需求，需要在好几张gpu上加载大模型，进一步导致大模型的冷启动问题。不同参数规模的大模型加载开销不同，但基本都要超过10分钟的开销。&lt;/p&gt;
&lt;p&gt;针对大模型的冷启动问题常用的解决思路如下&lt;/p&gt;
&lt;h3 id=&#34;预测流派&#34;&gt;预测流派&lt;/h3&gt;
&lt;p&gt;调度的根本原因在于系统请求量的变化，预测流派使用EWMA，ARIMA等方法&lt;font color=&#39;red&#39;&gt;粗粒度&lt;/font&gt;的提前预测下一时段的系统请求量，并提前计算好新的调度策略。&lt;/p&gt;
&lt;p&gt;&lt;font color=&#39;red&#39;&gt;粗粒度&lt;/font&gt;是按照小时来作为时间段的单位，预测下一时段（几个小时）的请求量是多少，然后在下一个时段中慢慢进行模型的加载（因为相比于小时，10min还是比较短的时间）。由于是提前预测好，并不是系统突然的请求量burst，所以可以消除冷启动带来的影响（等到请求量增大时，已经提前加载好了大模型）&lt;/p&gt;
&lt;p&gt;预测流派需要对系统请求量的变化有相对准确的预测，如果不准确会导致资源的浪费（多加载了大模型）/SLO 冲突（少加载了大模型）&lt;/p&gt;
&lt;p&gt;相关文献&lt;em&gt;&lt;strong&gt;《Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale》&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这个paper中还详细讨论了在下一个时段中慢慢进行模型加载的策略&lt;/p&gt;
&lt;p&gt;Naive方法：在得出新的调度策略后，直接进行改变&lt;/p&gt;
&lt;p&gt;Instance Utilization (LT-U)方法：考虑模型实例的使用率，当使用率增大超过某一个阈值时按照新的调度策略逐渐加载新的模型副本，但是不可以超过调度策略规定的值，相反，当使用率减少低于某一个阈值时按照新的调度策略逐渐卸载新的模型副本，但是不可以低于调度策略规定的值&lt;/p&gt;
&lt;p&gt;Instance utilization and ARIMA gap (LT-UA)方法：考虑模型实例的使用率，当使用率增大超过某一个阈值时按照新的调度策略逐渐加载新的模型副本，可以超过调度策略规定的值，相反，当使用率减少低于某一个阈值时按照新的调度策略逐渐卸载新的模型副本，可以低于调度策略规定的值。这个策略可以在预测不准的情况下，更智能的进行扩缩容&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Flash Attention MFU Calcuation</title>
      <link>https://theflash010.github.io/posts/flash_attention_mfu_calcuation/</link>
      <pubDate>Mon, 24 Mar 2025 10:39:17 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/flash_attention_mfu_calcuation/</guid>
      
      <description>&lt;h1 id=&#34;计算flash-attention中self-attention部分的mfu&#34;&gt;计算Flash Attention中Self-attention部分的MFU&lt;/h1&gt;
&lt;p&gt;前置知识&lt;/p&gt;
&lt;p&gt;模型算力利用率（Model FLOPs Utilization， MFU）和硬件算力利用率（Hardware FLOPs Utilization， HFU）是评估某一模型实现对芯片计算性能利用情况的常用指标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型算力利用率（MFU）是指模型一次前反向计算消耗的矩阵算力与机器算力的&lt;strong&gt;比值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;硬件算力利用率（HFU）是指考虑重计算后，模型一次前反向计算消耗的矩阵算力与机器算力的&lt;strong&gt;比值&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用公式来表达就是 &lt;strong&gt;MFU = model FLOPs per iteration/（GPU单卡算力*卡数*一次迭代时间）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;机器算力只需通过记录计算花费的时间，查询加速器的峰值算力进行计算&lt;/p&gt;
&lt;p&gt;FLOPs的计算相对复杂，需要手推公式&lt;/p&gt;
&lt;h2 id=&#34;flash-attention中self-attention的flops推导&#34;&gt;Flash Attention中Self-attention的FLOPs推导&lt;/h2&gt;
&lt;p&gt;Flash Attention的主要思想是从HBM拿到一份数据，对这份数据做尽可能多的计算操作（也就是提高&lt;strong&gt;arithmetic intensity&lt;/strong&gt;算术强度），来减少显存传输的开销&lt;/p&gt;
&lt;p&gt;Flash Attention对于一块数据会进行Q*K&lt;sup&gt;T&lt;/sup&gt;，softmax，score*V这三个操作。传统的Transformer是需要三遍pass，所有数据做完一个操作再做下一个操作，导致更多的显存传输&lt;/p&gt;
&lt;p&gt;以下对FLOPs进行推导&lt;/p&gt;
&lt;p&gt;对于使用Tiling的Flash Attention，外循环是Q，内循环是K，V，所以K和V的分块是相同的&lt;/p&gt;
&lt;p&gt;Flash Attention中不同块的数据可以并行处理（不同Q的数据可以并行，相同Q的需要串行），但是这并不影响总体的FLOPs，因为并行带来的好处是时间减少，实际的计算量并不会减少（除非改变算法）&lt;/p&gt;
&lt;p&gt;变量符号：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Name&lt;/th&gt;
          &lt;th&gt;Variables&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Batch_size&lt;/td&gt;
          &lt;td&gt;b&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Sequence_len&lt;/td&gt;
          &lt;td&gt;s&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Hidden_size&lt;/td&gt;
          &lt;td&gt;h&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;num_heads&lt;/td&gt;
          &lt;td&gt;n_h&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;number of tiling of Q&lt;/td&gt;
          &lt;td&gt;t_q&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;number of tiling of K&lt;/td&gt;
          &lt;td&gt;t_k&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先，input*（W&lt;sub&gt;Q&lt;/sub&gt; W&lt;sub&gt;K&lt;/sub&gt; W&lt;sub&gt;V&lt;/sub&gt; ）-&amp;gt; （Q，K，V）&lt;/p&gt;
&lt;p&gt;FLOPs_1=3（b，s，h）*（h，h）= 3*2bsh&lt;sup&gt;2&lt;/sup&gt;=6bsh&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;做完操作1后，首先多头注意力机制会对Q，K，V进行划分，（b，s，h）-&amp;gt; （b，n_h，s，h/n_h）；接着Flash Attention需要按照t_q，t_k进行分块，Q：（b，n_h，s，h/n_h）-&amp;gt;（t_q，b，n_h，s/t_q，h/n_h），K，V：（n_h，b，s，h/n_h）-&amp;gt;（t_k，b，n_h，s/t_k，h/n_h）。在分块结束后的计算量就是所有块的计算量的累加，所以后面对单个块进行追踪，考虑其FLOPs，按如下所示的计算流程&lt;/p&gt;
&lt;img src=&#34;.\image-20250324114222193.png&#34; alt=&#34;image-20250324114222193&#34; /&gt;
&lt;p&gt;2.1 Q*K&lt;sup&gt;T&lt;/sup&gt;的操作，操作结束后的矩阵形状为（b，n_h，s/t_q，s/t_k）&lt;/p&gt;
&lt;p&gt;FLOPs_2 =（b，n_h，s/t_q，h/n_h）*（b，n_h，s/t_k，h/n_h）&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Megatron_LM</title>
      <link>https://theflash010.github.io/posts/megatron_lm/</link>
      <pubDate>Fri, 21 Mar 2025 15:35:05 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/megatron_lm/</guid>
      
      <description>&lt;h1 id=&#34;megatron_lm中使用的的优化方案&#34;&gt;Megatron_LM中使用的的优化方案&lt;/h1&gt;
&lt;p&gt;Megatron_LM结合了DP，PP，TP，提出的叫PTD-P策略&lt;/p&gt;
&lt;p&gt;最外层是DP，然后一个DP内使用PP，一个PP内使用TP&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;img src=&#34;.\image-20250321153637863.png&#34; alt=&#34;image-20250321153637863&#34; /&gt;
&lt;h2 id=&#34;pp调度方面优化&#34;&gt;PP调度方面优化&lt;/h2&gt;
&lt;p&gt;使用了interleaved pipeline schedule版本的1F1B，通过增加流水线级数来减少气泡时间，具体可以看&lt;a href=&#34;https://theflash010.github.io/posts/pipeline_parallelism/&#34;&gt;《Pipeline Parallelism》&lt;/a&gt;这篇博客&lt;/p&gt;
&lt;img src=&#34;.\image-20250321154650026.png&#34; alt=&#34;image-20250321154650026&#34; /&gt;
&lt;h2 id=&#34;pptp通信方面的优化&#34;&gt;PP+TP通信方面的优化&lt;/h2&gt;
&lt;p&gt;前置知识：每个PP的stage使用的是DGX服务器（有8个A100），DGX内部GPU的通信是用NVLink的，DGX与外部通信是通过8个IB实现的，IB传输是点对点的，所以一次通信调用最多用一个IB设备，其余7个当然可以同时处理其他的点对点通信，但对于这次的通信调用起不到帮助。此外，Megatron_LM的TP如下&lt;/p&gt;
&lt;img src=&#34;.\image-20250321154546371.png&#34; alt=&#34;image-20250321154546371&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;然而，现有的PP+TP中，stage间的通信存在冗余。如下图所示&lt;/p&gt;
&lt;p&gt;现有的通信流程如下：TP组在执行完上图中MLP部分的Dropout后，每个rank都有完整的结果激活值，然后每个rank都将完整的激活值通过IB发送给下一个stage的DGX中对应的rank（这是跨节点的通信传输），所以其实每个rank使用IB传输都是相同的一份数据，存在冗余&lt;/p&gt;
&lt;p&gt;Megatron_LM优化了通信方案，让每个rank只传输1/8的数据给另一个stage对应rank结点，然后在下一个stage的内部使用all-gather重组激活值数据，由于stage内部使用的是NVLink，所以可以很快，这样相当于使用8个IB一起传输一份数据，不仅减少了通信量，而且提高了并行度&lt;/p&gt;
&lt;img src=&#34;.\image-20250321155831934.png&#34; alt=&#34;image-20250321155831934&#34; /&gt;</description>
      
    </item>
    
    <item>
      <title>Pipeline Parallelism</title>
      <link>https://theflash010.github.io/posts/pipeline_parallelism/</link>
      <pubDate>Wed, 19 Mar 2025 14:58:46 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/pipeline_parallelism/</guid>
      
      <description>&lt;h1 id=&#34;pp策略&#34;&gt;PP策略&lt;/h1&gt;
&lt;p&gt;PP的两个要素layer assignment（层划分） 和 scheduling strategy（调度策略）&lt;/p&gt;
&lt;p&gt;这里仅讨论调度策略，每个stage什么时候做前传，什么时候做反传&lt;/p&gt;
&lt;p&gt;流水线刷新（Pipeline Flush）是在 流水线并行（Pipeline Parallelism）中，为了确保所有设备（如 GPU）上的计算和优化器步骤（optimizer steps）同步执行，而采取的一种强制同步机制。（通过增加mirco-batch的数量，在stage数量不变的情况下，单个stage的处理时间变短，最后等待刷新的设备空闲时间短，可以减少流水线刷新的影响）&lt;/p&gt;
&lt;h2 id=&#34;naive版本&#34;&gt;Naive版本&lt;/h2&gt;
&lt;p&gt;naive的做法，一次将一个epoch的所有mini-batch注入流水线中，全部进行完前传后，进行反传。所有mini-batch反传结束后，统一进行梯度更新，这样相当于一个epoch只进行了一次梯度更新，epoch退化成了batch，所以训练效率低下（虽然流水资源利用率高，机器都在运行）&lt;/p&gt;
&lt;h2 id=&#34;gpipe版本&#34;&gt;GPipe版本&lt;/h2&gt;
&lt;p&gt;GPipe是在Naive的基础上换成m个mini-batch注入到流水中，也是全部前传后全部反传&lt;/p&gt;
&lt;p&gt;存在Pipeline flush问题，在一轮m个mini-batch之后，需要冲刷流水线，为下一轮做准备（通过增加micro-batch数量来减少流水线刷新影响）&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;img src=&#34;.\image-20250319143350643.png&#34; alt=&#34;image-20250319143350643&#34; /&gt;
&lt;h2 id=&#34;1f1b-版本论文版&#34;&gt;1F1B 版本（论文版）&lt;/h2&gt;
&lt;p&gt;来自论文&lt;em&gt;&lt;strong&gt;《PipeDream: Generalized Pipeline Parallelism for DNN Training》&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;会将mini-batch不断注入到流水中，边 正传 边 反传更新参数，同时严格要求stage必须交替一次正传一次反传&lt;/p&gt;
&lt;p&gt;不需要进行流水线刷新（除非流水线出问题进行修复时刷新流水线）&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;img src=&#34;.\image-20250319143535310.png&#34; alt=&#34;image-20250319143535310&#34; /&gt;
&lt;p&gt;但是很明显有很多问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如：当mini-batch2在反传时，mini-batch1已经修改了stage4的参数，那么mini-batch2本身应该按最初的参数进行计算梯度，现在被修改了，很明显不能直接用修改的参数进行反传。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解决上诉问题：1F1B会stash权重，存储不同的权重版本。所以面对mini-batch2的反传，所有stage不仅有最新的参数（由mini-batch1更新而来），还有最初版本的参数，从而支持梯度的计算。&lt;font color=&#39;red&#39;&gt;（对于worker的显存是否会带来非常大的额外开销？？？）&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如：当mini-batch2进行反传时，已经进行参数更新，那么mini-batch2的梯度对于模型训练是否还有效果呢？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者表示参数更新变化较慢，就算参数更新了，后面mini-batch的梯度还是对训练有效，做实验效果证明了这一点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如：对于mini-batch5（后续用mb5来代替）来说，正传存在使用权重版本不一致的问题，比如在stage1，mb5用的参数是mb1更新得到的，在stage2，mb2用的参数是mb2更新得到的，这种权重版本不一致是否带来问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者提出来其他技术来保证权重版本一致，为了解决这个问题，使用了Vertical  Sync，这个方法中一个stage会保留更多的权重版本，mini-batch5在前传的时候，会在不同stage里使用由mini-batch1更新得到的权重，比如stage2会额外保留mini-batch1更新后的权重，以便mini-batch5在前传时版本统一，等到mini-batch5反传结束时，不同stage上有关mini-batch1更新的权重版本就可以删掉了&lt;/p&gt;
&lt;p&gt;作者表示1F1B的默认语义中，并不包括Vertical  Sync技术，因为这个会带来额外的显存开销，版本不一致可能并不会影响训练效果&lt;/p&gt;
&lt;h3 id=&#34;显存开销的讨论&#34;&gt;显存开销的讨论&lt;/h3&gt;
&lt;p&gt;在某一个时刻，每个stage进行Weight stashing的额外显存开销只需要考虑当前存在的反传依赖所需的版本号&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;p&gt;考虑最右侧时刻&lt;/p&gt;
&lt;p&gt;stage1中mini-batch5,6,7,8均存在反传依赖，而且需要不同的版本号（5-&amp;gt;1，6-&amp;gt;2，7-&amp;gt;3，5-&amp;gt;4），所以需要保存四个版本权重&lt;/p&gt;
&lt;p&gt;stage4中mini-batch1的反传结束后，无需v0版本的权重，可以删除v0，保留v1，后续mini-batch2的正传也使用的是v1版本的权重，等mini-batch2反传结束，v1又可以删除，保留v2版本权重，从此反复，始终只需要保留一个版本权重&lt;/p&gt;
&lt;p&gt;假设一个完整的模型大小是N，本身流水并行，每个stage的权重是N/p，而从上述分析，每个stage不会保留超过p个版本的权重，所以每个stage显存不大于N，相比于数据并行，每个worker都是N还是有好处的。&lt;/p&gt;
&lt;img src=&#34;.\image-20250320100623997.png&#34; alt=&#34;image-20250320100623997&#34; /&gt;
&lt;h2 id=&#34;1f1b-版本工程版&#34;&gt;1F1B 版本（工程版）&lt;/h2&gt;
&lt;p&gt;由于版本权重带来的显存问题，工程上选择在反传时不进行梯度更新，等batch结束时进行同步更新，也就是工程上会使用流水刷新&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;img src=&#34;.\image-20250320173445048.png&#34; alt=&#34;image-20250320173445048&#34; /&gt;
&lt;h3 id=&#34;gpipe和1f1b的比较&#34;&gt;GPipe和1F1B的比较&lt;/h3&gt;
&lt;h4 id=&#34;气泡方面&#34;&gt;气泡方面&lt;/h4&gt;
&lt;p&gt;如果1F1B有pipeline flush（1F1B工程版），其实GPipe和1F1B的气泡时间是一样的，如下图所示&lt;/p&gt;
&lt;img src=&#34;.\image-20250320171443810.png&#34; alt=&#34;image-20250320171443810&#34; /&gt;
&lt;p&gt;具体计算公式如下&lt;/p&gt;
&lt;img src=&#34;.\image-20250320175154103.png&#34; alt=&#34;image-20250320175154103&#34; /&gt;
&lt;p&gt;但是对于1F1B（论文版）不需要流水线刷新，所以n个batch，GPipe需要刷新n-1次，1F1B不需要，气泡数量因此更少&lt;/p&gt;
&lt;h4 id=&#34;显存方面&#34;&gt;显存方面&lt;/h4&gt;
&lt;p&gt;考虑1F1B（工程版）&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Activation Memory Analysis</title>
      <link>https://theflash010.github.io/posts/activation_memory_analysis/</link>
      <pubDate>Tue, 18 Mar 2025 11:53:10 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/activation_memory_analysis/</guid>
      
      <description>&lt;h1 id=&#34;分析使用不同模型并行策略模型训练时激活值的显存占用情况&#34;&gt;分析使用不同模型并行策略，模型训练时&lt;font color=red&gt;激活值&lt;/font&gt;的显存占用情况&lt;/h1&gt;
&lt;p&gt;来源论文&lt;em&gt;&lt;strong&gt;《REDUCING ACTIVATION RECOMPUTATION IN LARGE TRANSFORMER  MODELS》&lt;/strong&gt;&lt;/em&gt;的分析&lt;/p&gt;
&lt;p&gt;分析前提&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Note that “activations” in this paper refers to any tensor that is created in the forward pass and is necessary for gradient computation during back-propagation. As a result, this excludes the main parameters of the model and optimizer state, but, for example, includes the mask used by the dropout operation.           （dropout mask是被包括在里面的）&lt;/li&gt;
&lt;li&gt;We also assume that the network and the activations are stored in a 16-bit floating point format and therefore each element requires 2 bytes for storage. The only exceptions are the dropout masks which only require a single byte per element.    (除了dropout mask是1byte外，其余都是2 bytes)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;baseline-不使用任何模型并行策略&#34;&gt;Baseline 不使用任何模型并行策略&lt;/h2&gt;
&lt;img src=&#34;.\image-20250317032832928.png&#34; alt=&#34;image-20250317032832928&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;上图是一个Transformer架构图，基于这个图分析显存占用情况（单layer）:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Encoder Decoder vs Decoder Only</title>
      <link>https://theflash010.github.io/posts/encoder-decoder_vs_decoder-only/</link>
      <pubDate>Wed, 08 Jan 2025 11:10:08 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/encoder-decoder_vs_decoder-only/</guid>
      
      <description>&lt;h1 id=&#34;encoder-decoder--vs--decoder-only&#34;&gt;Encoder-Decoder  VS  Decoder-Only&lt;/h1&gt;
&lt;h2 id=&#34;encoder-decoder&#34;&gt;&lt;mark&gt;Encoder-Decoder&lt;/mark&gt;&lt;/h2&gt;
&lt;h4 id=&#34;encoder&#34;&gt;Encoder&lt;/h4&gt;
&lt;p&gt;作用：将输入文本的信息进行提取&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在实现时使用 attention + add&amp;amp;norm + FNN为每一个单词都生成一个Encoder输出e&lt;del&gt;i&lt;/del&gt;(向量数据)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：e&lt;sub&gt;i&lt;/sub&gt;始终是指Encoder最顶层块的输出结果（如果Encoder有很多块的话）&lt;/p&gt;
&lt;h4 id=&#34;decoder&#34;&gt;Decoder&lt;/h4&gt;
&lt;p&gt;作用：利用Encoder的输出向量数据逐个生成结果&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;插入一个&amp;lt;bos&amp;gt;字符，表示开始生成&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假设当前已经推理出n个单词（不包括输入文本！！！），对第n个单词使用Masked Attention，生成第n个单词与前n-1个单词的自注意力分数，并计算出自注意力值d（向量数据），还有add&amp;amp;norm（暂不考虑）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将d与Encoder得到的所有输入单词的结果向量e&lt;sub&gt;i&lt;/sub&gt;进行cross attention，每个e&lt;sub&gt;i&lt;/sub&gt;会分别乘M&lt;sub&gt;k&lt;/sub&gt;，M&lt;sub&gt;v&lt;/sub&gt;矩阵，得到k&lt;sub&gt;i&lt;/sub&gt;，v&lt;sub&gt;i&lt;/sub&gt;向量，而d会乘以M&lt;sub&gt;q&lt;/sub&gt;矩阵，得到q&lt;sub&gt;d&lt;/sub&gt;向量，然后k&lt;sub&gt;i&lt;/sub&gt;，v&lt;sub&gt;i&lt;/sub&gt;与q&lt;sub&gt;d&lt;/sub&gt;进行attention计算，得出结果o（向量数据）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将o进行add&amp;amp;norm，FNN最后产生新的单词word，重复2，3，4步，直到生成&amp;lt;eos&amp;gt;表示结束生成&lt;/p&gt;
&lt;p&gt;注意：就算decoder有很多块，每一块中的cross attention中的e&lt;sub&gt;i&lt;/sub&gt;都是一样的，都是Encoder最顶层块的结果&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：就算decoder有很多块，每一块中的cross attention中的e&lt;sub&gt;i&lt;/sub&gt;都是一样的，都是Encoder最顶层块的结果&lt;/p&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;
&lt;p&gt;Encoder-Decoder架构会使用Encoder进行输入文本的信息提取，然后Decoder每次生成都会使用Encoder提取出的文本信息&lt;/p&gt;
&lt;h2 id=&#34;decoder-only&#34;&gt;&lt;mark&gt;Decoder-Only&lt;/mark&gt;&lt;/h2&gt;
&lt;h4 id=&#34;decoder-1&#34;&gt;Decoder&lt;/h4&gt;
&lt;p&gt;作用：不断使用attention来生成结果&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从文本输入首单词开始（注意这里不是插入&amp;lt;bos&amp;gt;），进行masked attention，但是在文本输入结束前的推理word是不会输出的，一直到文本输入最后一个单词开始才会把推理word输出，循环如此，直到生成&amp;lt;eos&amp;gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;tips：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实际上文本输入不需要推理，只需要计算这些输入单词的KV Cache就可以了，等到文本输入最后一个单词才用进行推理&lt;/li&gt;
&lt;li&gt;其实Decoder-Only分为两个阶段prefill和decode，其中prefill进行输入文本的KV Cache计算，decode进行推理（先prefill再decode）&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;总结-1&#34;&gt;总结&lt;/h4&gt;
&lt;p&gt;Decoder-Only架构没用Encoder进行输入文本的信息提取，而是直接进行推理，在推理的时候利用masked attention感知前面输入文本的内容，从而实现推理&lt;/p&gt;
&lt;img src=&#34;https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png&#34; alt=&#34;img&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
      
    </item>
    
    <item>
      <title>CUDA Study</title>
      <link>https://theflash010.github.io/posts/cuda_study/</link>
      <pubDate>Fri, 03 Jan 2025 14:02:03 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/cuda_study/</guid>
      
      <description>&lt;h2 id=&#34;heterogeneous-programming异构编程&#34;&gt;Heterogeneous Programming（异构编程）&lt;/h2&gt;
&lt;p&gt;用非单一架构的机器进行计算的编程模型，例如CPU与GPU融合&lt;/p&gt;
&lt;h4 id=&#34;asynchronous-simt-programming-model异步simt编程模型&#34;&gt;Asynchronous SIMT Programming Model（异步SIMT编程模型）&lt;/h4&gt;
&lt;p&gt;指CUDA程序在执行某些耗时操作（如GPU内核执行、内存拷贝等）时，不会等待这些操作完成，而是允许(CPU中)程序继续执行后续的命令或任务。这种机制使得CPU和GPU能够更高效地并行工作，提高了程序的整体性能。所谓异步Asyn,就是指多个设备之间步伐不一致，随便执行。同步Syn是指多个设备间需要步伐一致，当一个设备在运行时，其他设备需要等待它结束，再一起继续往下执行。&lt;/p&gt;
&lt;h2 id=&#34;compute-capability版本与cuda版本&#34;&gt;Compute Capability版本与CUDA版本&lt;/h2&gt;
&lt;p&gt;设备的Compute Capability由版本号表示，有时也称其“ SM 版本”。该版本号标识 GPU 硬件支持的特性，并由应用程序在运行时使用，以确定当前GPU上可用的硬件特性和指令。Compute Capability包括一个主要版本号X和一个次要版本号Y，用X.Y表示。CUDA 版本指的是 CUDA软件平台的版本。CUDA平台被应用开发人员用来创建那些可以运行在许多代GPU架构上的应用程序，包括未来尚未发明的 GPU架构。尽管CUDA平台的新版本通常会通过支持新GPU架构的Compute Capability版本来增加对于该架构的本地支持，但 CUDA平台的新版本通常也会包含软件功能，而这些是与硬件独立的。&lt;/p&gt;
&lt;h2 id=&#34;cuda-runtime&#34;&gt;Cuda Runtime&lt;/h2&gt;
&lt;p&gt;CUDA Runtime是NVIDIA提供的一个用于简化CUDA编程的高级API，它建立在CUDA Driver API之上，提供了一组更高层次的接口，旨在使开发者更容易使用CUDA进行并行编程。如cudaMalloc和cudaFree函数,允许开发者定义和启动核函数（kernel functions）。&lt;/p&gt;
&lt;h2 id=&#34;cuda-context&#34;&gt;Cuda Context&lt;/h2&gt;
&lt;p&gt;CUDA的上下文（Context）是CUDA编程中的一个重要概念，它代表了CUDA运行时（Runtime）与特定CUDA设备（通常是NVIDIA GPU）之间的关联和状态信息。当你使用CUDA Runtime API进行编程时，CUDA运行时会自动为每个CUDA设备创建一个上下文（除非你显式地请求在不同的线程中为每个设备创建独立的上下文）。一个context是为一个进程服务的，该进程内的所有线程共同使用这个context。&lt;/p&gt;
&lt;h4 id=&#34;primary-context&#34;&gt;primary context&lt;/h4&gt;
&lt;p&gt;在CUDA编程中，primary context（主上下文）是指与CUDA设备关联的第一个或默认的上下文(自动创建，用户不可见)。这个上下文在CUDA运行时（Runtime）初始化时自动创建，并且通常用于管理该设备上的大部分CUDA操作。它在应用程序的所有主机线程之间共享。、&lt;/p&gt;
&lt;h2 id=&#34;cuda-stream-cuda流&#34;&gt;Cuda Stream （Cuda流）&lt;/h2&gt;
&lt;p&gt;一般使用GPU加速，启动GPU内核或内存数据复制(CudaMemcpy)这类GPU操作是按顺序进行的，这实际是因为这些操作是由一个默认的流掌控的，如果想要让GPU的一些操作并发执行，就需要将不同操作放到不同的流上，流内部的操作顺序执行，不同流之间的操作异步执行，这就是流的概念。
CUDA流指的是由主机发出的在一个设备中执行的CUDA操作序列。一个CUDA流中的各个操作按照主机发布的次序执行；但来自两个不同CUDA流的操作不一定按照某个次序执行，有可能是并发或者交错地执行。可以将流类比成 CPU 编程中的“线程”的概念（注意不是 CUDA 编程概念中的线程）：同一个“线程”中的任务串行执行，不同“线程”可以并行执行。
默认流可以分为两种：legacy default stream 和 per-thread default stream。&lt;/p&gt;
&lt;h4 id=&#34;legacy-default-stream&#34;&gt;legacy default stream&lt;/h4&gt;
&lt;p&gt;默认情况下（无编译选项或者使用 nvcc 编译时加上 &amp;ndash;default-stream legacy 编译选项），每个设备（每张 GPU 卡）会创建一个默认流（NULL 流），称为 legacy default stream，该设备上所有不指定流或者指定默认流的操作全都放到这个流中。如果主机使用多线程，那么多线程的GPU操作，都会放到这个流中，共用这一个流。
注意：如果主机线程在来自不同流(自定义流)的两个命令之间向 legacy default stream 发出指令，那么这些来自不同流的命令不能同时运行。一句话总结：legacy default stream 会和所有非 non-blocking stream 产生同步。&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
