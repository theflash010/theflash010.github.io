<!DOCTYPE html>
<html lang="en-gb"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Flash Attention MFU Calcuation | NAN blog</title>
<meta property="og:title" content="Flash Attention MFU Calcuation | NAN blog" />
<meta name="twitter:title" content="Flash Attention MFU Calcuation | NAN blog" />
<meta itemprop="name" content="Flash Attention MFU Calcuation | NAN blog" />
<meta name="application-name" content="Flash Attention MFU Calcuation | NAN blog" />
<meta property="og:site_name" content="NAN blog" />

<meta name="description" content="Record of Learning Journey">
<meta itemprop="description" content="Record of Learning Journey" />
<meta property="og:description" content="Record of Learning Journey" />
<meta name="twitter:description" content="Record of Learning Journey" />

<meta property="og:locale" content="en-gb" />
<meta name="language" content="en-gb" />

  <link rel="alternate" hreflang="en-gb" href="https://theflash010.github.io/posts/flash_attention_mfu_calcuation/" title="English" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2025-03-24T10:39:17&#43;0800 />
    <meta property="article:published_time" content=2025-03-24T10:39:17&#43;0800 />
    <meta property="og:url" content="https://theflash010.github.io/posts/flash_attention_mfu_calcuation/" />

    
    <meta property="og:article:author" content="Nan Z" />
    <meta property="article:author" content="Nan Z" />
    <meta name="author" content="Nan Z" />
    
    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Flash Attention MFU Calcuation",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2025-03-24",
        "description": "",
        "wordCount":  96 ,
        "mainEntityOfPage": "True",
        "dateModified": "2025-03-24",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "NAN blog"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.145.0">

    
    <meta property="og:url" content="https://theflash010.github.io/posts/flash_attention_mfu_calcuation/">
  <meta property="og:site_name" content="NAN blog">
  <meta property="og:title" content="Flash Attention MFU Calcuation">
  <meta property="og:description" content="计算Flash Attention中Self-attention部分的MFU 前置知识
模型算力利用率（Model FLOPs Utilization， MFU）和硬件算力利用率（Hardware FLOPs Utilization， HFU）是评估某一模型实现对芯片计算性能利用情况的常用指标
模型算力利用率（MFU）是指模型一次前反向计算消耗的矩阵算力与机器算力的比值 硬件算力利用率（HFU）是指考虑重计算后，模型一次前反向计算消耗的矩阵算力与机器算力的比值 用公式来表达就是 MFU = model FLOPs per iteration/（GPU单卡算力*卡数*一次迭代时间）
机器算力只需通过记录计算花费的时间，查询加速器的峰值算力进行计算
FLOPs的计算相对复杂，需要手推公式
Flash Attention中Self-attention的FLOPs推导 Flash Attention的主要思想是从HBM拿到一份数据，对这份数据做尽可能多的计算操作（也就是提高arithmetic intensity算术强度），来减少显存传输的开销
Flash Attention对于一块数据会进行Q*KT，softmax，score*V这三个操作。传统的Transformer是需要三遍pass，所有数据做完一个操作再做下一个操作，导致更多的显存传输
以下对FLOPs进行推导
对于使用Tiling的Flash Attention，外循环是Q，内循环是K，V，所以K和V的分块是相同的
Flash Attention中不同块的数据可以并行处理（不同Q的数据可以并行，相同Q的需要串行），但是这并不影响总体的FLOPs，因为并行带来的好处是时间减少，实际的计算量并不会减少（除非改变算法）
变量符号：
Name Variables Batch_size b Sequence_len s Hidden_size h num_heads n_h number of tiling of Q t_q number of tiling of K t_k 首先，input*（WQ WK WV ）-&gt; （Q，K，V）
FLOPs_1=3（b，s，h）*（h，h）= 3*2bsh2=6bsh2
做完操作1后，首先多头注意力机制会对Q，K，V进行划分，（b，s，h）-&gt; （b，n_h，s，h/n_h）；接着Flash Attention需要按照t_q，t_k进行分块，Q：（b，n_h，s，h/n_h）-&gt;（t_q，b，n_h，s/t_q，h/n_h），K，V：（n_h，b，s，h/n_h）-&gt;（t_k，b，n_h，s/t_k，h/n_h）。在分块结束后的计算量就是所有块的计算量的累加，所以后面对单个块进行追踪，考虑其FLOPs，按如下所示的计算流程
2.1 Q*KT的操作，操作结束后的矩阵形状为（b，n_h，s/t_q，s/t_k）
FLOPs_2 =（b，n_h，s/t_q，h/n_h）*（b，n_h，s/t_k，h/n_h）T">
  <meta property="og:locale" content="en_gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-24T10:39:17+08:00">
    <meta property="article:modified_time" content="2025-03-24T10:39:17+08:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Flash Attention MFU Calcuation">
  <meta name="twitter:description" content="计算Flash Attention中Self-attention部分的MFU 前置知识
模型算力利用率（Model FLOPs Utilization， MFU）和硬件算力利用率（Hardware FLOPs Utilization， HFU）是评估某一模型实现对芯片计算性能利用情况的常用指标
模型算力利用率（MFU）是指模型一次前反向计算消耗的矩阵算力与机器算力的比值 硬件算力利用率（HFU）是指考虑重计算后，模型一次前反向计算消耗的矩阵算力与机器算力的比值 用公式来表达就是 MFU = model FLOPs per iteration/（GPU单卡算力*卡数*一次迭代时间）
机器算力只需通过记录计算花费的时间，查询加速器的峰值算力进行计算
FLOPs的计算相对复杂，需要手推公式
Flash Attention中Self-attention的FLOPs推导 Flash Attention的主要思想是从HBM拿到一份数据，对这份数据做尽可能多的计算操作（也就是提高arithmetic intensity算术强度），来减少显存传输的开销
Flash Attention对于一块数据会进行Q*KT，softmax，score*V这三个操作。传统的Transformer是需要三遍pass，所有数据做完一个操作再做下一个操作，导致更多的显存传输
以下对FLOPs进行推导
对于使用Tiling的Flash Attention，外循环是Q，内循环是K，V，所以K和V的分块是相同的
Flash Attention中不同块的数据可以并行处理（不同Q的数据可以并行，相同Q的需要串行），但是这并不影响总体的FLOPs，因为并行带来的好处是时间减少，实际的计算量并不会减少（除非改变算法）
变量符号：
Name Variables Batch_size b Sequence_len s Hidden_size h num_heads n_h number of tiling of Q t_q number of tiling of K t_k 首先，input*（WQ WK WV ）-&gt; （Q，K，V）
FLOPs_1=3（b，s，h）*（h，h）= 3*2bsh2=6bsh2
做完操作1后，首先多头注意力机制会对Q，K，V进行划分，（b，s，h）-&gt; （b，n_h，s，h/n_h）；接着Flash Attention需要按照t_q，t_k进行分块，Q：（b，n_h，s，h/n_h）-&gt;（t_q，b，n_h，s/t_q，h/n_h），K，V：（n_h，b，s，h/n_h）-&gt;（t_k，b，n_h，s/t_k，h/n_h）。在分块结束后的计算量就是所有块的计算量的累加，所以后面对单个块进行追踪，考虑其FLOPs，按如下所示的计算流程
2.1 Q*KT的操作，操作结束后的矩阵形状为（b，n_h，s/t_q，s/t_k）
FLOPs_2 =（b，n_h，s/t_q，h/n_h）*（b，n_h，s/t_k，h/n_h）T">


    

    <link rel="canonical" href="https://theflash010.github.io/posts/flash_attention_mfu_calcuation/">
    <link href="/style.min.2d921c18cf1ec555ffc03d59a8adc211c402c68c930c27d6a0c306ab175a8d09.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="https://theflash010.github.io/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
    
</head>
<body data-theme = "auto" class="notransition">

<script src="/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="https://theflash010.github.io/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/posts/">
                        Posts
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/pages/about/">
                        About
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Flash Attention MFU Calcuation</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-03-24T10:39:17&#43;08:00" itemprop="datePublished"> 24 Mar 2025 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b>Table of Contents</b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#flash-attention中self-attention的flops推导">Flash Attention中Self-attention的FLOPs推导</a></li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <h1 id="计算flash-attention中self-attention部分的mfu">计算Flash Attention中Self-attention部分的MFU</h1>
<p>前置知识</p>
<p>模型算力利用率（Model FLOPs Utilization， MFU）和硬件算力利用率（Hardware FLOPs Utilization， HFU）是评估某一模型实现对芯片计算性能利用情况的常用指标</p>
<ul>
<li>模型算力利用率（MFU）是指模型一次前反向计算消耗的矩阵算力与机器算力的<strong>比值</strong></li>
<li>硬件算力利用率（HFU）是指考虑重计算后，模型一次前反向计算消耗的矩阵算力与机器算力的<strong>比值</strong></li>
</ul>
<p>用公式来表达就是 <strong>MFU = model FLOPs per iteration/（GPU单卡算力*卡数*一次迭代时间）</strong></p>
<p>机器算力只需通过记录计算花费的时间，查询加速器的峰值算力进行计算</p>
<p>FLOPs的计算相对复杂，需要手推公式</p>
<h2 id="flash-attention中self-attention的flops推导">Flash Attention中Self-attention的FLOPs推导</h2>
<p>Flash Attention的主要思想是从HBM拿到一份数据，对这份数据做尽可能多的计算操作（也就是提高<strong>arithmetic intensity</strong>算术强度），来减少显存传输的开销</p>
<p>Flash Attention对于一块数据会进行Q*K<sup>T</sup>，softmax，score*V这三个操作。传统的Transformer是需要三遍pass，所有数据做完一个操作再做下一个操作，导致更多的显存传输</p>
<p>以下对FLOPs进行推导</p>
<p>对于使用Tiling的Flash Attention，外循环是Q，内循环是K，V，所以K和V的分块是相同的</p>
<p>Flash Attention中不同块的数据可以并行处理（不同Q的数据可以并行，相同Q的需要串行），但是这并不影响总体的FLOPs，因为并行带来的好处是时间减少，实际的计算量并不会减少（除非改变算法）</p>
<p>变量符号：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Name</th>
          <th>Variables</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Batch_size</td>
          <td>b</td>
      </tr>
      <tr>
          <td style="text-align: left">Sequence_len</td>
          <td>s</td>
      </tr>
      <tr>
          <td style="text-align: left">Hidden_size</td>
          <td>h</td>
      </tr>
      <tr>
          <td style="text-align: left">num_heads</td>
          <td>n_h</td>
      </tr>
      <tr>
          <td style="text-align: left">number of tiling of Q</td>
          <td>t_q</td>
      </tr>
      <tr>
          <td style="text-align: left">number of tiling of K</td>
          <td>t_k</td>
      </tr>
  </tbody>
</table>
<ol>
<li>
<p>首先，input*（W<sub>Q</sub> W<sub>K</sub> W<sub>V</sub> ）-&gt; （Q，K，V）</p>
<p>FLOPs_1=3（b，s，h）*（h，h）= 3*2bsh<sup>2</sup>=6bsh<sup>2</sup></p>
</li>
<li>
<p>做完操作1后，首先多头注意力机制会对Q，K，V进行划分，（b，s，h）-&gt; （b，n_h，s，h/n_h）；接着Flash Attention需要按照t_q，t_k进行分块，Q：（b，n_h，s，h/n_h）-&gt;（t_q，b，n_h，s/t_q，h/n_h），K，V：（n_h，b，s，h/n_h）-&gt;（t_k，b，n_h，s/t_k，h/n_h）。在分块结束后的计算量就是所有块的计算量的累加，所以后面对单个块进行追踪，考虑其FLOPs，按如下所示的计算流程</p>
<img src=".\image-20250324114222193.png" alt="image-20250324114222193" />
<p>2.1 Q*K<sup>T</sup>的操作，操作结束后的矩阵形状为（b，n_h，s/t_q，s/t_k）</p>
<p>FLOPs_2 =（b，n_h，s/t_q，h/n_h）*（b，n_h，s/t_k，h/n_h）<sup>T</sup></p>
<p>​	       = b*n_h*2（h/n_h）*（s/t_q）*（s/t_k）</p>
<p>​		= 2bhs<sup>2</sup>/（t_q*t_k）</p>
<p>2.2 选出最大的x的计算量忽略不计，考虑图中计算di的FLOPs（和传统的softmax操作一样）</p>
<p>FLOPs_3 = 2*b*n_h*（s/t_q）*（s/t_k）</p>
<p>​		= 2b*n_h*s<sup>2</sup>/（t_q*t_k）</p>
<p>2.3 考虑图中计算oi的FLOPs</p>
<p>FLOPs_4 =（前一部分忽略）（b，n_h，s/t_q，s/t_k）*（b，n_h，s/t_k，h/n_h）<sup>T</sup></p>
<p>​		=b*n_h*2（h/n_h）*（s/t_q）*（s/t_k）</p>
<p>​		=2bhs<sup>2</sup>/（t_q*t_k）</p>
</li>
<li>
<p>在2中的计算仅仅是一个块的FLOPs，需要乘以（t_q*t_k）倍，所以总的计算公式</p>
<p>FLOPs=FLOPs_1+（t_q*t_k）*(FLOPs_2+FLOPs_3+FLOPs_4)=<font color="red">6bsh<sup>2</sup>+4bhs<sup>2</sup>+2bs<sup>2</sup>n_h </font></p>
</li>
</ol>
<p>综上，Flash Attention的Self-attention部分的FLOPs=<strong><font color="red">6bsh<sup>2</sup>+4bhs<sup>2</sup>+2bs<sup>2</sup>n_h </font></strong></p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/theflash010" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="https://space.bilibili.com/106734387?spm_id_from=333.1007.0.0" target="_blank" rel="noopener noreferrer me"
    title="Bilibili">
    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" xmlns="http://www.w3.org/2000/svg">
    <rect x="1.3333" y="6" width="21.333" height="15.333" rx="4" ry="4"/>
    <path d="m8 12.4v1.2"/>
    <path d="m16 12.4v1.2"/>
    <path d="m5.8853 2.6667 2.6667 2.6667"/>
    <path d="m18.115 2.6667-2.6667 2.6667"/>
</svg>
</a>
<a href="/index.xml" target="_blank" rel="noopener noreferrer me"
    title="Rss">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 11a9 9 0 0 1 9 9" />
    <path d="M4 4a16 16 0 0 1 16 16" />
    <circle cx="5" cy="19" r="1" />
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2025 Nan Z.
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer><a href="#" title="Go to top" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    




    
    
        
    

    
    
        
    



    
    <script src="https://theflash010.github.io/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js" integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30&#43;lSNuSkl4QXuNyy8="></script>

    

</body>
</html>
