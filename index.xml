<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NAN blog</title>
    <link>https://theflash010.github.io/</link>
    <description>Recent content on NAN blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Tue, 18 Mar 2025 11:53:10 +0800</lastBuildDate><atom:link href="https://theflash010.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Activation Memory Analysis</title>
      <link>https://theflash010.github.io/posts/activation-memory-analysis/</link>
      <pubDate>Tue, 18 Mar 2025 11:53:10 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/activation-memory-analysis/</guid>
      
      <description>&lt;h1 id=&#34;分析使用不同模型并行策略模型训练时激活值的显存占用情况&#34;&gt;分析使用不同模型并行策略，模型训练时**&lt;font color=red&gt;激活值&lt;/font&gt;**的显存占用情况&lt;/h1&gt;
&lt;p&gt;分析前提&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Note that “activations” in this paper refers to any tensor that is created in the forward pass and is necessary for gradient computation during back-propagation. As a result, this excludes the main parameters of the model and optimizer state, but, for example, includes the mask used by the dropout operation.           （dropout mask是被包括在里面的）&lt;/li&gt;
&lt;li&gt;We also assume that the network and the activations are stored in a 16-bit floating point format and therefore each element requires 2 bytes for storage. The only exceptions are the dropout masks which only require a single byte per element.    (除了dropout mask是1byte外，其余都是2 bytes)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;baseline-不使用任何模型并行策略&#34;&gt;Baseline 不使用任何模型并行策略&lt;/h2&gt;
&lt;img src=&#34;C:\Users\Legend\AppData\Roaming\Typora\typora-user-images\image-20250317032832928.png&#34; alt=&#34;image-20250317032832928&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;上图是一个Transformer架构图，基于这个图分析显存占用情况（单layer）:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Encoder Decoder vs Decoder Only</title>
      <link>https://theflash010.github.io/posts/encoder-decoder-vs-decoder-only/</link>
      <pubDate>Wed, 08 Jan 2025 11:10:08 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/encoder-decoder-vs-decoder-only/</guid>
      
      <description>&lt;h1 id=&#34;encoder-decoder--vs--decoder-only&#34;&gt;Encoder-Decoder  VS  Decoder-Only&lt;/h1&gt;
&lt;h2 id=&#34;encoder-decoder&#34;&gt;&lt;mark&gt;Encoder-Decoder&lt;/mark&gt;&lt;/h2&gt;
&lt;h4 id=&#34;encoder&#34;&gt;Encoder&lt;/h4&gt;
&lt;p&gt;作用：将输入文本的信息进行提取&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在实现时使用 attention + add&amp;amp;norm + FNN为每一个单词都生成一个Encoder输出e&lt;del&gt;i&lt;/del&gt;(向量数据)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：e&lt;sub&gt;i&lt;/sub&gt;始终是指Encoder最顶层块的输出结果（如果Encoder有很多块的话）&lt;/p&gt;
&lt;h4 id=&#34;decoder&#34;&gt;Decoder&lt;/h4&gt;
&lt;p&gt;作用：利用Encoder的输出向量数据逐个生成结果&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;插入一个&amp;lt;bos&amp;gt;字符，表示开始生成&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假设当前已经推理出n个单词（不包括输入文本！！！），对第n个单词使用Masked Attention，生成第n个单词与前n-1个单词的自注意力分数，并计算出自注意力值d（向量数据），还有add&amp;amp;norm（暂不考虑）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将d与Encoder得到的所有输入单词的结果向量e&lt;sub&gt;i&lt;/sub&gt;进行cross attention，每个e&lt;sub&gt;i&lt;/sub&gt;会分别乘M&lt;sub&gt;k&lt;/sub&gt;，M&lt;sub&gt;v&lt;/sub&gt;矩阵，得到k&lt;sub&gt;i&lt;/sub&gt;，v&lt;sub&gt;i&lt;/sub&gt;向量，而d会乘以M&lt;sub&gt;q&lt;/sub&gt;矩阵，得到q&lt;sub&gt;d&lt;/sub&gt;向量，然后k&lt;sub&gt;i&lt;/sub&gt;，v&lt;sub&gt;i&lt;/sub&gt;与q&lt;sub&gt;d&lt;/sub&gt;进行attention计算，得出结果o（向量数据）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将o进行add&amp;amp;norm，FNN最后产生新的单词word，重复2，3，4步，直到生成&amp;lt;eos&amp;gt;表示结束生成&lt;/p&gt;
&lt;p&gt;注意：就算decoder有很多块，每一块中的cross attention中的e&lt;sub&gt;i&lt;/sub&gt;都是一样的，都是Encoder最顶层块的结果&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：就算decoder有很多块，每一块中的cross attention中的e&lt;sub&gt;i&lt;/sub&gt;都是一样的，都是Encoder最顶层块的结果&lt;/p&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;
&lt;p&gt;Encoder-Decoder架构会使用Encoder进行输入文本的信息提取，然后Decoder每次生成都会使用Encoder提取出的文本信息&lt;/p&gt;
&lt;h2 id=&#34;decoder-only&#34;&gt;&lt;mark&gt;Decoder-Only&lt;/mark&gt;&lt;/h2&gt;
&lt;h4 id=&#34;decoder-1&#34;&gt;Decoder&lt;/h4&gt;
&lt;p&gt;作用：不断使用attention来生成结果&lt;/p&gt;
&lt;p&gt;过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从文本输入首单词开始（注意这里不是插入&amp;lt;bos&amp;gt;），进行masked attention，但是在文本输入结束前的推理word是不会输出的，一直到文本输入最后一个单词开始才会把推理word输出，循环如此，直到生成&amp;lt;eos&amp;gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;tips：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实际上文本输入不需要推理，只需要计算这些输入单词的KV Cache就可以了，等到文本输入最后一个单词才用进行推理&lt;/li&gt;
&lt;li&gt;其实Decoder-Only分为两个阶段prefill和decode，其中prefill进行输入文本的KV Cache计算，decode进行推理（先prefill再decode）&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;总结-1&#34;&gt;总结&lt;/h4&gt;
&lt;p&gt;Decoder-Only架构没用Encoder进行输入文本的信息提取，而是直接进行推理，在推理的时候利用masked attention感知前面输入文本的内容，从而实现推理&lt;/p&gt;
&lt;img src=&#34;https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png&#34; alt=&#34;img&#34; style=&#34;zoom:50%;&#34; /&gt;</description>
      
    </item>
    
    <item>
      <title>Introduce</title>
      <link>https://theflash010.github.io/about/</link>
      <pubDate>Fri, 03 Jan 2025 14:30:31 +0800</pubDate>
      
      <guid>https://theflash010.github.io/about/</guid>
      
      <description>&lt;ul&gt;
&lt;li&gt;graduated from Jilin University (CS)&lt;/li&gt;
&lt;li&gt;pursuing a PhD at Zhejiang University(CS) at present&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title>CUDA Study</title>
      <link>https://theflash010.github.io/posts/cuda-study/</link>
      <pubDate>Fri, 03 Jan 2025 14:02:03 +0800</pubDate>
      
      <guid>https://theflash010.github.io/posts/cuda-study/</guid>
      
      <description>&lt;h2 id=&#34;heterogeneous-programming异构编程&#34;&gt;Heterogeneous Programming（异构编程）&lt;/h2&gt;
&lt;p&gt;用非单一架构的机器进行计算的编程模型，例如CPU与GPU融合&lt;/p&gt;
&lt;h4 id=&#34;asynchronous-simt-programming-model异步simt编程模型&#34;&gt;Asynchronous SIMT Programming Model（异步SIMT编程模型）&lt;/h4&gt;
&lt;p&gt;指CUDA程序在执行某些耗时操作（如GPU内核执行、内存拷贝等）时，不会等待这些操作完成，而是允许(CPU中)程序继续执行后续的命令或任务。这种机制使得CPU和GPU能够更高效地并行工作，提高了程序的整体性能。所谓异步Asyn,就是指多个设备之间步伐不一致，随便执行。同步Syn是指多个设备间需要步伐一致，当一个设备在运行时，其他设备需要等待它结束，再一起继续往下执行。&lt;/p&gt;
&lt;h2 id=&#34;compute-capability版本与cuda版本&#34;&gt;Compute Capability版本与CUDA版本&lt;/h2&gt;
&lt;p&gt;设备的Compute Capability由版本号表示，有时也称其“ SM 版本”。该版本号标识 GPU 硬件支持的特性，并由应用程序在运行时使用，以确定当前GPU上可用的硬件特性和指令。Compute Capability包括一个主要版本号X和一个次要版本号Y，用X.Y表示。CUDA 版本指的是 CUDA软件平台的版本。CUDA平台被应用开发人员用来创建那些可以运行在许多代GPU架构上的应用程序，包括未来尚未发明的 GPU架构。尽管CUDA平台的新版本通常会通过支持新GPU架构的Compute Capability版本来增加对于该架构的本地支持，但 CUDA平台的新版本通常也会包含软件功能，而这些是与硬件独立的。&lt;/p&gt;
&lt;h2 id=&#34;cuda-runtime&#34;&gt;Cuda Runtime&lt;/h2&gt;
&lt;p&gt;CUDA Runtime是NVIDIA提供的一个用于简化CUDA编程的高级API，它建立在CUDA Driver API之上，提供了一组更高层次的接口，旨在使开发者更容易使用CUDA进行并行编程。如cudaMalloc和cudaFree函数,允许开发者定义和启动核函数（kernel functions）。&lt;/p&gt;
&lt;h2 id=&#34;cuda-context&#34;&gt;Cuda Context&lt;/h2&gt;
&lt;p&gt;CUDA的上下文（Context）是CUDA编程中的一个重要概念，它代表了CUDA运行时（Runtime）与特定CUDA设备（通常是NVIDIA GPU）之间的关联和状态信息。当你使用CUDA Runtime API进行编程时，CUDA运行时会自动为每个CUDA设备创建一个上下文（除非你显式地请求在不同的线程中为每个设备创建独立的上下文）。一个context是为一个进程服务的，该进程内的所有线程共同使用这个context。&lt;/p&gt;
&lt;h4 id=&#34;primary-context&#34;&gt;primary context&lt;/h4&gt;
&lt;p&gt;在CUDA编程中，primary context（主上下文）是指与CUDA设备关联的第一个或默认的上下文(自动创建，用户不可见)。这个上下文在CUDA运行时（Runtime）初始化时自动创建，并且通常用于管理该设备上的大部分CUDA操作。它在应用程序的所有主机线程之间共享。、&lt;/p&gt;
&lt;h2 id=&#34;cuda-stream-cuda流&#34;&gt;Cuda Stream （Cuda流）&lt;/h2&gt;
&lt;p&gt;一般使用GPU加速，启动GPU内核或内存数据复制(CudaMemcpy)这类GPU操作是按顺序进行的，这实际是因为这些操作是由一个默认的流掌控的，如果想要让GPU的一些操作并发执行，就需要将不同操作放到不同的流上，流内部的操作顺序执行，不同流之间的操作异步执行，这就是流的概念。
CUDA流指的是由主机发出的在一个设备中执行的CUDA操作序列。一个CUDA流中的各个操作按照主机发布的次序执行；但来自两个不同CUDA流的操作不一定按照某个次序执行，有可能是并发或者交错地执行。可以将流类比成 CPU 编程中的“线程”的概念（注意不是 CUDA 编程概念中的线程）：同一个“线程”中的任务串行执行，不同“线程”可以并行执行。
默认流可以分为两种：legacy default stream 和 per-thread default stream。&lt;/p&gt;
&lt;h4 id=&#34;legacy-default-stream&#34;&gt;legacy default stream&lt;/h4&gt;
&lt;p&gt;默认情况下（无编译选项或者使用 nvcc 编译时加上 &amp;ndash;default-stream legacy 编译选项），每个设备（每张 GPU 卡）会创建一个默认流（NULL 流），称为 legacy default stream，该设备上所有不指定流或者指定默认流的操作全都放到这个流中。如果主机使用多线程，那么多线程的GPU操作，都会放到这个流中，共用这一个流。
注意：如果主机线程在来自不同流(自定义流)的两个命令之间向 legacy default stream 发出指令，那么这些来自不同流的命令不能同时运行。一句话总结：legacy default stream 会和所有非 non-blocking stream 产生同步。&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
